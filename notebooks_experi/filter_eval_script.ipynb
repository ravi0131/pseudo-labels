{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is for writing the filtering and evaluation script. \n",
    "The best way to do this, is to first filter all bboxes and then save them for each frame in every scene. \n",
    "Then, we peform evaluation on it by loading the bboxes of a given frame and its gt_boxes and calculating the metrics for them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Add the src directory to the Python path\n",
    "sys.path.append(os.path.abspath(\"..\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First the code to process each frame, each scene, and all scenes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import stats_filter\n",
    "import pandas as pd\n",
    "from prototype_utils import bboxes_df_to_numpy_corners\n",
    "from config import CONFIG\n",
    "\n",
    "def process_frame(input_frame_path: str, frame_id: str, scene_save_path: str):\n",
    "    \"\"\"\n",
    "    Args: \n",
    "        input_frame_path: path to load the bboxes' dataframe for the given frame. \n",
    "        frame_id: id of the frame of interst\n",
    "        scene_save_path: the path to save the processed frame. Frame will be saved as feather file at scene_save_path/frame_id.feather\n",
    "                        ensure that this directory exists before calling this function\n",
    "        \n",
    "    \"\"\"\n",
    "    #filter bboxes\n",
    "    bboxes_df = pd.read_feather(input_frame_path)\n",
    "    bboxes_df['aspect_ratio'] = bboxes_df['box_width'] / bboxes_df['box_length']\n",
    "    bboxes_df['area'] = bboxes_df['box_width'] * bboxes_df['box_length']\n",
    "    _, normal_bboxes_df, _ = stats_filter.filter_by_aspect_ratio(bboxes_df, 'aspect_ratio', CONFIG)\n",
    "    square_filter_df = stats_filter.filter_squares_by_area(normal_bboxes_df, 'aspect_ratio', 'area', CONFIG)\n",
    "    rect_filter_df = stats_filter.filter_rects_by_area(normal_bboxes_df, 'aspect_ratio', 'area', CONFIG)\n",
    "\n",
    "    combined_df = pd.concat([square_filter_df, rect_filter_df]).drop_duplicates()\n",
    "    combined_df.reset_index(drop=True, inplace=True)\n",
    "    #apply nms\n",
    "    _, selected_idxes = stats_filter.apply_nms_on_pseudo_labels(bboxes_df_to_numpy_corners(combined_df), CONFIG['NMS_IOU_THRESHOLD'])\n",
    "    nms_df = combined_df.iloc[selected_idxes]\n",
    "\n",
    "    nms_df.reset_index(drop=True, inplace=True)\n",
    "    if not os.path.exists(scene_save_path):\n",
    "        raise ValueError(f\"Path {scene_save_path} does not exist. Please create the directory before calling this function\")\n",
    "    nms_df.to_feather(os.path.join(scene_save_path, f\"{frame_id}.feather\"))\n",
    "    \n",
    "def process_scene(input_scene_path: str, scene_id: str, output_save_path: str):\n",
    "    \"\"\"\n",
    "    Args: \n",
    "        scene_id: the id of the scene\n",
    "        input_scene_path: the path to the scene  {os.listdir(input_scene_path) will return the frames}\n",
    "        output_save_path: the base path to save each scene. Each frame will be saved as feather file at output_save_path/scene_id/frame_id.feather\n",
    "    \"\"\"\n",
    "    scene_save_path = os.path.join(output_save_path, scene_id)\n",
    "    os.makedirs(scene_save_path, exist_ok=True)\n",
    "    try:\n",
    "        for input_frame in os.listdir(input_scene_path):\n",
    "            input_frame_path = os.path.join(input_scene_path, input_frame)\n",
    "            frame_id = input_frame.split(\".\")[0]\n",
    "            process_frame(input_frame_path, frame_id, scene_save_path)\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing scene {scene_id} : {e}\")\n",
    "        \n",
    "        \n",
    "def process_all_scenes(input_path: str, output_save_path: str):\n",
    "    \"\"\"\n",
    "    Args: \n",
    "        input_path: the path to the scenes {os.listdir(input_path) will return the scenes}\n",
    "    \"\"\"\n",
    "    os.makedirs(output_save_path, exist_ok=True)\n",
    "    for scene in os.listdir(input_path):\n",
    "        input_scene_path = os.path.join(input_path, scene)\n",
    "        process_scene(input_scene_path, scene, output_save_path)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we load the intput_data_path(bbox src) and define output file path, refine bboxes and save it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from config import CONFIG\n",
    "home = os.path.join(os.path.expanduser(\"~\"), CONFIG['HOME_PATH'][CONFIG['OS']])\n",
    "\n",
    "\n",
    "if CONFIG['ROI']:\n",
    "    input_pl_path = os.path.join(home, *CONFIG['BBOX_FILE_PATHS']['ROI']) # pl = pseudo labels\n",
    "    filtered_pl_output_path = os.path.join(home, *CONFIG['FILTERED_BBOX_FILE_PATHS']['ROI'])\n",
    "\n",
    "else:\n",
    "    input_pl_path = os.path.join(home, *CONFIG['BBOX_FILE_PATHS']['FULL_RANGE']) \n",
    "    filtered_pl_output_path = os.path.join(home, *CONFIG['FILTERED_BBOX_FILE_PATHS']['FULL_RANGE'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "process_all_scenes(input_pl_path, filtered_pl_output_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we load bboxes, its corresponding gt_box and calculate metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from av2.datasets.sensor.av2_sensor_dataloader import AV2SensorDataLoader\n",
    "from pathlib import Path\n",
    "from av2.structures.sweep import Sweep\n",
    "dataset_path = Path(os.path.join(home, \"dataset\", \"av2\", \"train\"))\n",
    "av2 = AV2SensorDataLoader(data_dir=dataset_path, labels_dir=dataset_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from prototype_utils import filter_cuboids_by_roi, extract_face_corners\n",
    "from eval_metrics import compute_matches, compute_ap2, reorder_by_iou\n",
    "from typing import Dict\n",
    "\n",
    "def calculate_metrics_frame(input_pl_frame_path: str, frame_id: str,scene_id: str, scene_save_path: str, av2:AV2SensorDataLoader,  config: Dict):\n",
    "    try: \n",
    "        #load ground truth\n",
    "        if config['ROI']: \n",
    "            cuboids = av2.get_labels_at_lidar_timestamp(scene_id, int(frame_id)).vertices_m\n",
    "            filtered_cuboids = filter_cuboids_by_roi(cuboids, config)\n",
    "            gt_corners = extract_face_corners(filtered_cuboids)\n",
    "        else:\n",
    "            cuboids = av2.get_labels_at_lidar_timestamp(scene_id, int(frame_id)).vertices_m\n",
    "            gt_corners = extract_face_corners(cuboids)\n",
    "        \n",
    "        #load pseudo_labels\n",
    "        pseudo_labels_df = pd.read_feather(input_pl_frame_path)\n",
    "        pseudo_labels_corners = bboxes_df_to_numpy_corners(pseudo_labels_df)\n",
    "        # print(f\"scene_save_path: {scene_save_path}\")\n",
    "        \n",
    "        #compute matches\n",
    "        _ , pseudo_label_matches, overlaps = compute_matches(gt_corners, pseudo_labels_corners)\n",
    "        \n",
    "        # pseudo_label_matches = reorder_by_iou(pseudo_label_matches, overlaps)\n",
    "        \n",
    "        gt_length = len(gt_corners)\n",
    "        num_of_predictions = len(pseudo_labels_corners)\n",
    "        \n",
    "        mAP, precisions, recalls, precision, recall =compute_ap2(pseudo_label_matches,gt_length,num_of_predictions)\n",
    "        \n",
    "        # print(f\" type of mAP: {type(mAP)} \\n type of precision: {type(precision)} \\n type of recall: {type(recall)} \\n type of precisions: {type(precisions)} \\n type of recalls: {type(recalls)}\")\n",
    "        # print(f\"precisions.shape: {precisions.shape} \\nrecalls.shape: {recalls.shape}\")\n",
    "        results_df = pd.DataFrame({\n",
    "            \"mAP\": [mAP],\n",
    "            \"precision\": [precision],\n",
    "            \"recall\": [recall],\n",
    "            \"precisions\": [precisions],\n",
    "            \"recalls\": [recalls],\n",
    "            \"pseudo_label_matches\": [pseudo_label_matches],\n",
    "            \"num_of_predictions\": [num_of_predictions],\n",
    "            \"gt_length\": [gt_length],\n",
    "        })\n",
    "        \n",
    "        if not os.path.exists(scene_save_path):\n",
    "            raise ValueError(f\"Path {scene_save_path} does not exist. Please create the directory before calling this function\")\n",
    "        \n",
    "        frame_save_path = os.path.join(scene_save_path, f\"{frame_id}.feather\")\n",
    "        results_df.to_feather(frame_save_path)    \n",
    "    except Exception as e:\n",
    "        print(f\"shape of filtered_cuboids: {filtered_cuboids.shape} for scene_id: {scene_id} and frame_id: {frame_id}\")\n",
    "        print(f\"Error: {e} caught because Frame: {frame_id} in scene: {scene_id} does not have enough ground truth labels in ROI\") \n",
    "        \n",
    "        \n",
    "def calculate_metrics_scene(input_pl_path: str, scene_id: str, base_save_path: str, av2: AV2SensorDataLoader):\n",
    "    scene_save_path = os.path.join(base_save_path, scene_id)\n",
    "    os.makedirs(scene_save_path, exist_ok=True)\n",
    "    # count = 0\n",
    "    for frame in os.listdir(input_pl_path): # frame is timsstamp.feather\n",
    "        # count += 1\n",
    "        input_frame_path = os.path.join(input_pl_path, frame)\n",
    "        frame_id = frame.split(\".\")[0]\n",
    "        # print(f\"processing frame {frame}\")\n",
    "        # print(f\"loading pseudo_labels from {input_frame_path}\")\n",
    "        # print(f\"scene_save_path: {scene_save_path}\")\n",
    "        calculate_metrics_frame(input_frame_path, frame_id, scene_id, scene_save_path, av2, CONFIG)\n",
    "        # if count == 1:\n",
    "        #     break\n",
    "    \n",
    "\n",
    "def calculate_metrics_all_per_frame(pseudo_labels_path: str, av2: AV2SensorDataLoader, base_save_path):\n",
    "    \"\"\"\n",
    "    Calculate the metrics for all the scenes\n",
    "    \"\"\"\n",
    "    os.makedirs(base_save_path, exist_ok=True)\n",
    "    # count = 0 \n",
    "    for scene_id in os.listdir(pseudo_labels_path):\n",
    "        # count += 1\n",
    "        input_scene_path = os.path.join(pseudo_labels_path, scene_id)\n",
    "        # print(f\"loading pseudo_labels from {input_scene_path} for scne_id: {scene_id}\" )\n",
    "        calculate_metrics_scene(input_scene_path, scene_id, base_save_path, av2)\n",
    "        # if count == 1:\n",
    "        #     break\n",
    "\n",
    "\n",
    "def calculate_total_metrics(base_save_path: str):\n",
    "    \"\"\"\n",
    "    Calculate the total metrics for all the scenes\n",
    "    \"\"\"\n",
    "    mAPs = []\n",
    "    precisions = []\n",
    "    recalls = []\n",
    "    for scene_id in os.listdir(base_save_path):\n",
    "        scene_path = os.path.join(base_save_path, scene_id)\n",
    "        for frame in os.listdir(scene_path):\n",
    "            frame_path = os.path.join(scene_path, frame)\n",
    "            results_df = pd.read_feather(frame_path)\n",
    "            mAPs.append(results_df['mAP'][0])\n",
    "            precisions.append(results_df['precision'][0])\n",
    "            recalls.append(results_df['recall'][0])\n",
    "    mAP = sum(mAPs)/len(mAPs)\n",
    "    precision = sum(precisions)/len(precisions)\n",
    "    recall = sum(recalls)/len(recalls)\n",
    "    return mAP, precision, recall"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate metrics for all filtered and unfiltered pseudo-labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "if CONFIG['ROI']:\n",
    "    metrics_normal_output_path = os.path.join(home, *CONFIG['METRICS_FILE_PATHS']['ROI']['NORMAL'])\n",
    "    metrics_filtered_output_path = os.path.join(home, *CONFIG['METRICS_FILE_PATHS']['ROI']['FILTERED'])\n",
    "else:\n",
    "    metrics_normal_output_path = os.path.join(home, *CONFIG['METRICS_FILE_PATHS']['FULL_RANGE']['NORMAL'])\n",
    "    metrics_filtered_output_path = os.path.join(home, *CONFIG['METRICS_FILE_PATHS']['FULL_RANGE']['FILTERED'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of filtered_cuboids: (0,) for scene_id: ff52c01e-3d7b-32b1-b6a1-bcff3459ccdd and frame_id: 315968341560625000\n",
      "Error: too many indices for array: array is 1-dimensional, but 3 were indexed caught because Frame: 315968341560625000 in scene: ff52c01e-3d7b-32b1-b6a1-bcff3459ccdd does not have enough ground truth labels in ROI\n",
      "shape of filtered_cuboids: (0,) for scene_id: ff52c01e-3d7b-32b1-b6a1-bcff3459ccdd and frame_id: 315968341660158000\n",
      "Error: too many indices for array: array is 1-dimensional, but 3 were indexed caught because Frame: 315968341660158000 in scene: ff52c01e-3d7b-32b1-b6a1-bcff3459ccdd does not have enough ground truth labels in ROI\n",
      "shape of filtered_cuboids: (0,) for scene_id: ff52c01e-3d7b-32b1-b6a1-bcff3459ccdd and frame_id: 315968341760354000\n",
      "Error: too many indices for array: array is 1-dimensional, but 3 were indexed caught because Frame: 315968341760354000 in scene: ff52c01e-3d7b-32b1-b6a1-bcff3459ccdd does not have enough ground truth labels in ROI\n",
      "shape of filtered_cuboids: (0,) for scene_id: ff52c01e-3d7b-32b1-b6a1-bcff3459ccdd and frame_id: 315968341859887000\n",
      "Error: too many indices for array: array is 1-dimensional, but 3 were indexed caught because Frame: 315968341859887000 in scene: ff52c01e-3d7b-32b1-b6a1-bcff3459ccdd does not have enough ground truth labels in ROI\n",
      "shape of filtered_cuboids: (0,) for scene_id: ff52c01e-3d7b-32b1-b6a1-bcff3459ccdd and frame_id: 315968341960083000\n",
      "Error: too many indices for array: array is 1-dimensional, but 3 were indexed caught because Frame: 315968341960083000 in scene: ff52c01e-3d7b-32b1-b6a1-bcff3459ccdd does not have enough ground truth labels in ROI\n",
      "shape of filtered_cuboids: (0,) for scene_id: ff52c01e-3d7b-32b1-b6a1-bcff3459ccdd and frame_id: 315968342059600000\n",
      "Error: too many indices for array: array is 1-dimensional, but 3 were indexed caught because Frame: 315968342059600000 in scene: ff52c01e-3d7b-32b1-b6a1-bcff3459ccdd does not have enough ground truth labels in ROI\n"
     ]
    }
   ],
   "source": [
    "calculate_metrics_all_per_frame(input_pl_path, av2, metrics_normal_output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of filtered_cuboids: (0,) for scene_id: ff52c01e-3d7b-32b1-b6a1-bcff3459ccdd and frame_id: 315968341560625000\n",
      "Error: too many indices for array: array is 1-dimensional, but 3 were indexed caught because Frame: 315968341560625000 in scene: ff52c01e-3d7b-32b1-b6a1-bcff3459ccdd does not have enough ground truth labels in ROI\n",
      "shape of filtered_cuboids: (0,) for scene_id: ff52c01e-3d7b-32b1-b6a1-bcff3459ccdd and frame_id: 315968341660158000\n",
      "Error: too many indices for array: array is 1-dimensional, but 3 were indexed caught because Frame: 315968341660158000 in scene: ff52c01e-3d7b-32b1-b6a1-bcff3459ccdd does not have enough ground truth labels in ROI\n",
      "shape of filtered_cuboids: (0,) for scene_id: ff52c01e-3d7b-32b1-b6a1-bcff3459ccdd and frame_id: 315968341760354000\n",
      "Error: too many indices for array: array is 1-dimensional, but 3 were indexed caught because Frame: 315968341760354000 in scene: ff52c01e-3d7b-32b1-b6a1-bcff3459ccdd does not have enough ground truth labels in ROI\n",
      "shape of filtered_cuboids: (0,) for scene_id: ff52c01e-3d7b-32b1-b6a1-bcff3459ccdd and frame_id: 315968341859887000\n",
      "Error: too many indices for array: array is 1-dimensional, but 3 were indexed caught because Frame: 315968341859887000 in scene: ff52c01e-3d7b-32b1-b6a1-bcff3459ccdd does not have enough ground truth labels in ROI\n",
      "shape of filtered_cuboids: (0,) for scene_id: ff52c01e-3d7b-32b1-b6a1-bcff3459ccdd and frame_id: 315968341960083000\n",
      "Error: too many indices for array: array is 1-dimensional, but 3 were indexed caught because Frame: 315968341960083000 in scene: ff52c01e-3d7b-32b1-b6a1-bcff3459ccdd does not have enough ground truth labels in ROI\n",
      "shape of filtered_cuboids: (0,) for scene_id: ff52c01e-3d7b-32b1-b6a1-bcff3459ccdd and frame_id: 315968342059600000\n",
      "Error: too many indices for array: array is 1-dimensional, but 3 were indexed caught because Frame: 315968342059600000 in scene: ff52c01e-3d7b-32b1-b6a1-bcff3459ccdd does not have enough ground truth labels in ROI\n"
     ]
    }
   ],
   "source": [
    "calculate_metrics_all_per_frame(filtered_pl_output_path, av2, metrics_filtered_output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mAP: 0.0295080541906726 \n",
      "precision: 0.052353910811642  \n",
      "recall: 0.225127005381434\n"
     ]
    }
   ],
   "source": [
    "mAP, precision, recall = calculate_total_metrics(metrics_normal_output_path)\n",
    "print(f\"mAP: {mAP} \\nprecision: {precision}  \\nrecall: {recall}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mAP_refined: 0.032271806002957315 \n",
      "precision_refined: 0.07312734458333706 \n",
      "recall_refined: 0.16503157281820421\n"
     ]
    }
   ],
   "source": [
    "mAP_refined, precision_refined, recall_refined = calculate_total_metrics(metrics_filtered_output_path) \n",
    "\n",
    "print(f\"mAP_refined: {mAP_refined} \\nprecision_refined: {precision_refined} \\nrecall_refined: {recall_refined}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9.366093048447519"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "((mAP_refined - mAP)/mAP)*100"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pixo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
